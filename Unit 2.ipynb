{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unit 2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTV14zXScxku3Z3Sfzb8Kz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"USz7sCsRxJB3"},"source":["# Unit 2\n","In this section we look at conditional probability."]},{"cell_type":"code","metadata":{"id":"db7y3zdkz8GG"},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bha98NEMLSF6"},"source":["# The Condition of Bayesville\n","From the example in the video, we discussed the probability of having a disease. An individual Person1 tested positive for a rare disease (maybe 1% of people have it). The test is known to be $95%$ accurate, which leaves room for error. Some key takeaways from the discussion:\n","- Even though the test was positive, it was still unlikey that the individual had it.\n","- There was a higher chance of error for false positives than false negatives.\n","- It lead to some questions:\n","\n","We need to distinguish between the probability of having a disease- event $A$- given a postive test result- event $B$- and the probability of testing positive given that one has the disease:\n","\\begin{equation}\n","  P(A|B)\\neq P(B|A)\n","\\end{equation}\n","\n","Whenever new information is gathered- by observation of gathering of new evidence or data- our beliefs must be updated in light of the new data. Conditional probability is the concept that addresses this fundamental question: how should we update our beliefs in light of the evidence we observe? This gives rise to the eqution shown, or $\\text{Baye's Rule}$. Baye's rule alongisde the $\\text{law of total probability}$ can be used to solve many problems. \n","\n","## Thinking Conditionally\n","It is useful to think that _all probabilities are conditional_, because there is always background knowledge or assumptions built in. Furthermore, conditional probability allows complex problems to be decompsed into moree manageable pieces.  "]},{"cell_type":"markdown","metadata":{"id":"PVHpFKrNOib_"},"source":["# Definition and Intuition\n","If $A$ and $B$ are events with $P(B)>0$, then the _conditional probability_ of $A$ given $B$ is:\n","\\begin{equation}\n","  P(A|B)=\\frac{A\\cap B}{B}\n","\\end{equation}\n","We are looking at $A$ given $B$, thus $A$ is the event whose uncertainty we would like to update in light of new evidence $B$ that has been given. \n","- $P(A)$ is the $prior$ probability of $A$: before updating basedd on evidence.\n","- $P(A|B)$ is the $posterior$ probability of $A$: after updating basedd on evidence.\n","\n","### Two Cards Example\n","A standard deck of 52 playing cards is shuffled well. Two cards are drawn randomly, one at a time without replacement. Let $A$ be the event that the first card is a heart, and $B$ be the event that the second card is red. Find $P(A|B)$ and $P(B|A)$.\\\n","\\\n","To solve this problem remember to also consider what we have learnt up to this ppoint, namely the naive and general definitions of % and the counting and sampling methods.\\\n","\\\n","By the naive definition and the multiplication rule:\n","\\begin{equation}\n","  P(A\\cap B)=\\frac{13*25}{52*51}=\\frac{25}{204}\n","\\end{equation}\n","This is true because there are 13 cards out of 52 that are hearts and, if the card is a hearts it is red, so there are 25 (not 26) out of 51 cards thereafter which would satisfy event B. \\\n","$P(A)$ is $\\frac{13}{52}=\\frac{1}{4}$. \\\n","$P(B)$ is $P(B)=\\frac{26*51}{52*51}$ because for the second card can be any 26 reds of the 52 cards. For the first, it can be any of the other 52 cards that is not chosen for the second card. The multiplication rule does not require chronological ordering, thus can be expressed as given. Finally:\n","\\begin{equation}\n","  P(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{25/204}{1/2}=\\frac{25}{102}\n","\\end{equation}\n","\\begin{equation}\n","  P(B|A)=\\frac{P(B\\cap A)}{P(A)}=\\frac{25/204}{1/4}=\\frac{25}{51}\n","\\end{equation}\n","- Make sure the the order of $(|)$ is correct. Confusing $A$ and $B$ is known as _prosecutor's fallacy_.\n","- The chronological order in which the cards were chosen _does not_ dictate whether or not conditional %'s can be looked at. This means that both $P(A|B)$ and $P(B|A)$ make sense. "]},{"cell_type":"markdown","metadata":{"id":"susNakMPi3WL"},"source":["### Intuition with Pebble World\n","Given a finite sample space with events $A$ and $B$ and with _total mass 1_. Pebbles in $A$ are crossed out and $B$ is bolded. \\\n","\n","| ~0~  ~0~  ~0~ | \\\n","| ~0~  ~__0__~  __0__ | \\\n","| __0__  __0__  __0__ | \\\n","\n","Leaning that $B$ occured, pebbles in $B^c$ can be removed. $P(A\\cap B)$ is the total mass of the pebbles remaining in $A$. Then it can be _renormalized_ s.t the remaining mass has a total of 1, by dividing by $P(B)$. "]},{"cell_type":"markdown","metadata":{"id":"YUf4pGuklLHE"},"source":["### Intuition with Frequentist Interpretation\n","Imagine repeating an experiment many times, randomly generating a long list of observed outcomes, each of them represented by a string of twenty-four 0's and 1's.$B$ is the event that the first digit is 1 and $A$ is the event that the second digit is 1. The conditional probability of $A$ given $B$ can then be thought of in a natural way: it is the fraction of times that $A$ occurs, restricting attention to the trials where $B$ occurs. Conditioning on $B$, we circle all the repetitions where $B$ occurred, and then we look at the fraction of circled repetitions in which event $A$ also occurred. \\\n","\\\n","In symbols, let $n_B, n_B, n_{AB}$ be the number of occurrences of $A,B,A\\cap B$ respectively in a large number $n$ of repetitions of the experiment. The frequentist interpretation is that\n","\\begin{equation}\n","  P(A)\\approx \\frac{n_A}{n}, P(B)\\approx \\frac{n_B}{n}, P(A\\cap B)\\approx \\frac{n_AB}{n}\n","\\end{equation}\n","From this frequentist view, $P(A|B)$ can be interpreted as $n_{AB}/n_B$ which is equal to $(n_{AB}/n)/(n_B/n)$ which translates to $P(A\\cap B)/P(B)$.\n"]},{"cell_type":"markdown","metadata":{"id":"1zhCm_IDrHYO"},"source":["## Baye's Rule and the Law of Total Probability\n","There are consequences of the simple definition of conditional % (which is a ratio of two %'s).\n","### Consequence 1\n","For events $A$ and $B$ with positive %'s.\n","\\begin{equation}\n","  P(A\\cap B)=P(B)P(A|B)=P(A)P(B|A)\n","\\end{equation}\n","This argument seems circiular at first, because $P(A|B)$ was defined in terms of $P(A\\cap B)$, but it is useful. It allows the finding of conditional %'s w/o the need to go back to the definition.\n"]},{"cell_type":"code","metadata":{"id":"CnqDjFofi2kC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7A2Z3jQ5Oh2C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tez1pD4gVzSr"},"source":["\n"],"execution_count":null,"outputs":[]}]}